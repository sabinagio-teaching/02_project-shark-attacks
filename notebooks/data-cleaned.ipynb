{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c1bffa",
   "metadata": {},
   "source": [
    "# PROJECT 02 : SHARK ATTACKS\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a804f",
   "metadata": {},
   "source": [
    "### BRIEF : data analysis of global shark attacks for a business idea\n",
    "You will initially examine the Shark Attack dataset, understanding its structure and formulating a hypothesis or several hypotheses about the data. \n",
    " We hypothesize that shark attacks are more common in certain locations and peak during specific months.\n",
    "We define a Business Case, such as \n",
    "\n",
    "* ‚ÄúAs a company that sells medical products, I want to identify destinations with high shark attack rates.‚Äù\n",
    "* ‚ÄúAs a company providing supply transportation services, I want to know when and where shark attacks peak to plan the safe transport of medical supplies to hospitals.‚Äù\n",
    "\n",
    " Throughout the project, we will use Python and the pandas library to apply at least five data cleaning techniques to handle missing values, duplicates, and formatting inconsistencies. After cleaning, we will perform basic exploratory data analysis to validate our hypotheses and extract insights. \n",
    "\n",
    "#### üìù BUSINESS IDEA ‚Äî 3 Bullet Points\n",
    "\n",
    "* Problem to Solve: Coastal hospitals and emergency response teams are not always prepared with the right medical supplies during periods of high shark-attack frequency. This business solves the problem by predicting when and where attacks are most likely, so medical supplies can be stocked in advance.\n",
    "\n",
    "* Business Concept: Use historical shark attack data to create global heatmaps and seasonal risk forecasts. Then provide pharmaceutical products (painkillers, antibiotics, blood bags, emergency kits) and transportation support to hospitals and ambulances near high-risk beaches.\n",
    "\n",
    "* Data Used to Profit: The business will analyze Country, Date/Month, Gender, Age, Fatality, and Type of Injury to identify high-risk locations, peak attack months, and most common injury types. This allows optimized supply production, targeted sales, and efficient delivery to the areas that need it most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736f644",
   "metadata": {},
   "source": [
    "## üåÄCOLUMNS TO CLEAN : \n",
    "-----\n",
    "\n",
    "**- COUNTRY (global comparison between countries to invest in more)** @Blanca\n",
    "\n",
    "    * we just take the country column and make sure every country name is accurately named\n",
    "    * We are gonna check the column of COUNTRY and make sure every country name is correct\n",
    "    * We remove NULL COUNTRY data rows that dont have any country\n",
    "    * We need to make sure that the name of COUNTRY  is capitalized and written the same way for example :\n",
    "        United States of Amercia == USA == US\n",
    "        it has to have the same name and consistent!\n",
    "\n",
    "**- DATE ( MONTH + YEAR )** @Blanca\n",
    "\n",
    "    * split the date into day - month columns and only use month column\n",
    "    * interpret months which have shark attacks happen the most\n",
    "\n",
    "**- GENDER ( F or M )** @Cecilia\n",
    "\n",
    "    * We check unique values and make it so it is only two values F or M and deleted all rows that have other values\n",
    "    * we noticed mostly M get attacked\n",
    "    * percentages Male to Female victims\n",
    "    * We remove NULL DATE data rows that dont have a date or that the date doesnt include a month and a year\n",
    "    * We are gonna check the column of DATE and seperate it into three columns DAY + MONTH + YEAR\n",
    "    * We verify that the new YEAR column matches with the old YEAR column and keep the ones that match\n",
    "        * NEW YEAR COLUMN is the one split from the DATE column\n",
    "        * OLD YEAR COLUMN is the one already existing in the original sheet\n",
    "    * Once we finish comparing the new YEAR column vs old YEAR column and we find them not matching on some data rows. we remove the none matching ones so we keep clean data of accurate years\n",
    "    * We remove the DAY and OLD YEAR columns\n",
    "    * We are gonna keep the MONTH and matching YEAR\n",
    "\n",
    "**- AGE (victims age ranges)** @Samia\n",
    "\n",
    "    * majority of victims survive\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and 40+) \n",
    "    * keep in mind complications depending on age when getting treated\n",
    "    * percentages of victims based on age ranges\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and elders 40+) \n",
    "    * note that there are complications depending on age\n",
    "\n",
    "**- FATALITY ( Y or N )** @Cecilia\n",
    "    * depends on the column 'Type' of injury and if it includes death or high severity\n",
    "    * mostly survived \n",
    "    * for the pharamaceutical logistics & transportation of injured people to the hospital\n",
    "    * assumption we have a percentage of survivals highest and we use it to sell for the\n",
    "    * We check unique values and make sure it is only two values Y or N and deleted all rows that have other values\n",
    "    * assumption we have a high percentage of survivals and we use it to sell the idea to profit from selling products to hospitals\n",
    "\n",
    "**- INJURY TYPE** @Samia\n",
    "\n",
    "    * clean the type of injury by severity\n",
    "    * seperate the injury type into different severity\n",
    "    * seperate the injury type into different body parts\n",
    "    * treatment depends on type of injury and thus the supplies as well\n",
    "\n",
    "-------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9f7f7",
   "metadata": {},
   "source": [
    "### IMPORT UTILS and INIT from SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src import utils\n",
    "from src import init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff922e82",
   "metadata": {},
   "source": [
    "-----\n",
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\raw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\raw.csv\")\n",
    "\n",
    "# Option 1: Display the entire DataFrame (all rows and columns)\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "\n",
    "#the full table\n",
    "print(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837e2e3",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Country' CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ce029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba694fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTRY\n",
    "\n",
    "df = df.dropna(subset=[\"Country\"])\n",
    "df[\"Country\"] = df[\"Country\"].str.strip().str.title()\n",
    "\n",
    "country_replacements = {\n",
    "    \"United States Of America\": \"USA\",\n",
    "    \"Usa\": \"USA\",\n",
    "    \"Us\": \"USA\",\n",
    "    \"U.s.\": \"USA\",\n",
    "    \"United Kingdom\": \"UK\",\n",
    "    \"England\": \"UK\",\n",
    "    \"Uk\":\"UK\",\n",
    "    \"Brasil\": \"Brazil\",\n",
    "    \"M√©xico\": \"Mexico\",\n",
    "}\n",
    "\n",
    "df[\"Country\"] = df[\"Country\"].replace(country_replacements)\n",
    "df = df[df[\"Country\"] != \"Italy / Croatia\"]\n",
    "\n",
    "df[\"Country\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d7dd4",
   "metadata": {},
   "source": [
    "----------\n",
    "### 'Date' CLEANING AND SEPERATE INTO DD MM YYYY COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b25199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40213119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data to practice Date cleaning and seperating\n",
    "data = {\n",
    "    'date': [\n",
    "        '12 05 2023',\n",
    "        '23 07-2021',\n",
    "        '05-08 2022',\n",
    "        '17-11-2020'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Regex pattern to match all variations\n",
    "pattern = r'(?P<DD>\\d{2})[- ](?P<MM>\\d{2})[- ](?P<YYYY>\\d{4})'\n",
    "\n",
    "# Extract DD, MM, YYYY into new columns\n",
    "df[['DD', 'MM', 'YYYY']] = df['date'].str.extract(pattern)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# DataFrame with the 'Date' column\n",
    "df = pd.read_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\raw.csv\") # or pd.DataFrame({'Date': [...your list...]})\n",
    "\n",
    "# Function to parse dates-------------------------------------------------------------------------------------------------------------------\n",
    "def parse_date(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Remove 'Reported ' prefix\n",
    "    date_str = re.sub(r'^Reported\\s+', '', date_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Patterns to match\n",
    "    patterns = [\n",
    "        r'(?P<DD>\\d{1,2})[- ](?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',  # DD MMM YYYY\n",
    "        r'(?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',                      # MMM-YYYY\n",
    "        r'(?P<YYYY>\\d{4})',                                           # YYYY only\n",
    "    ]\n",
    "    \n",
    "    for pat in patterns:\n",
    "        match = re.match(pat, date_str)\n",
    "        if match:\n",
    "            return match.groupdict()\n",
    "    \n",
    "    return {'DD': None, 'MM': None, 'YYYY': None}\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Call Function to the column\n",
    "parsed_dates = df['Date'].apply(parse_date).apply(pd.Series)\n",
    "\n",
    "# Merge back to original DataFrame\n",
    "df = pd.concat([df, parsed_dates], axis=1)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301f9be",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Sex' CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a368dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa980d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ba081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN , F, M\n",
    "df['Sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b595985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Sex'] = df['Sex'].replace({\n",
    "    'm' : 'M', \n",
    "    'f' : 'F', \n",
    "    'Male' : 'M', \n",
    "    'Female' : 'F'})\n",
    "\n",
    "df['Sex'] = df['Sex'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dropna (subset = ['Sex'], inplace= True)\n",
    "df['Sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c1b69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb01f67f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183a57f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6ec1b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd50e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].info()\n",
    "#238 non-null object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e197f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812493ff",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Age' CLEANED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ed4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Age'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\raw.csv\")\n",
    "\n",
    "def clean_age(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove common unwanted characters\n",
    "    value = value.replace(\"years\", \"\").replace(\"ca.\", \"\").replace(\"about\", \"\").strip()\n",
    "\n",
    "    # Words that indicate no age\n",
    "    if value in [\"n/a\", \"na\", \"none\", \"?\", \"\", \"unknown\"]:\n",
    "        return np.nan\n",
    "\n",
    "    # Teen keyword (assume average age 15)\n",
    "    if \"teen\" in value:\n",
    "        return 15\n",
    "\n",
    "    # mid-20s -> 25\n",
    "    if \"mid\" in value and \"20\" in value:\n",
    "        return 25\n",
    "\n",
    "    # Capture numbers if present\n",
    "    nums = re.findall(r\"\\d+\", value)\n",
    "\n",
    "    if len(nums) == 1:\n",
    "        return int(nums[0])\n",
    "    \n",
    "    # If multiple numbers like \"20/30\", \"24 & 35\", \"16 to 18\", take the average\n",
    "    if len(nums) >= 2:\n",
    "        nums = [int(n) for n in nums]\n",
    "        return sum(nums) / len(nums)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Apply cleaning function\n",
    "df['Age_clean'] = df['Age'].apply(clean_age)\n",
    "\n",
    "# Convert to numeric properly\n",
    "df['Age_clean'] = pd.to_numeric(df['Age_clean'], errors='coerce')\n",
    "\n",
    "# Fill missing values with median\n",
    "median_age = df['Age_clean'].median()\n",
    "df['Age_clean'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Confirm results\n",
    "print(df['Age_clean'].head(20))\n",
    "print(\"Median age used:\", median_age)\n",
    "\n",
    "# (Optional) Save cleaned file\n",
    "df.to_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3490f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     238\n",
       "unique     72\n",
       "top        17\n",
       "freq       14\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].describe().round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454433af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# DATA .csv\n",
    "df = pd.read_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\raw.csv\")\n",
    "# def Function : clean_age(value=age)\n",
    "def clean_age(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless text\n",
    "    value = value.replace(\"years\", \"\").replace(\"year\", \"\").replace(\"ca.\", \"\").replace(\"about\", \"\").strip()\n",
    "\n",
    "    # Words meaning no age\n",
    "    if value in [\"n/a\", \"na\", \"none\", \"?\", \"\", \"unknown\"]:\n",
    "        return np.nan\n",
    "\n",
    "    # Teen keyword (approx.)\n",
    "    if \"teen\" in value:\n",
    "        return 15\n",
    "\n",
    "    # mid-20s ‚Üí 25\n",
    "    if \"mid\" in value and \"20\" in value:\n",
    "        return 25\n",
    "\n",
    "    # Extract numbers\n",
    "    nums = re.findall(r\"\\d+\", value)\n",
    "\n",
    "    # Single number\n",
    "    if len(nums) == 1:\n",
    "        return int(nums[0])\n",
    "    \n",
    "    # Multiple numbers -> average them (20/30, 23 & 20, 16 to 18)\n",
    "    if len(nums) >= 2:\n",
    "        nums = [int(n) for n in nums]\n",
    "        return sum(nums) / len(nums)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Apply cleaner\n",
    "df['Age_clean'] = df['Age'].apply(clean_age)\n",
    "\n",
    "# Convert to numeric\n",
    "df['Age_clean'] = pd.to_numeric(df['Age_clean'], errors='coerce')\n",
    "\n",
    "# Fill missing with median\n",
    "median_age = df['Age_clean'].median()\n",
    "df['Age_clean'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Convert to integer (no floats)\n",
    "df['Age_clean'] = df['Age_clean'].round().astype(int)\n",
    "\n",
    "# Show result\n",
    "print(df[['Age', 'Age_clean']].head(20))\n",
    "print(\"Median age used:\", median_age)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\cleaned.csv\", index=False)\n",
    "\n",
    "# Check result\n",
    "print(\"\\n\",df[\"Age_clean\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaffe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = df['Age_clean'].describe(percentiles=[0.25, 0.5, 0.75])\n",
    "\n",
    "print(\"Age Percentile Statistics:\")\n",
    "print(f\"Min:   {int(percentiles['min'])}\")\n",
    "print(f\"25%:   {int(percentiles['25%'])}\")\n",
    "print(f\"50% (Median): {int(percentiles['50%'])}\")\n",
    "print(f\"75%:   {int(percentiles['75%'])}\")\n",
    "print(f\"Max:   {int(percentiles['max'])}\")\n",
    "\n",
    "# the majority were people up to 26 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef536ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original Age column with the cleaned integer values\n",
    "df['Age'] = df['Age_clean']\n",
    "\n",
    "# Optionally, drop the temporary Age_clean column\n",
    "df.drop(columns=['Age_clean'], inplace=True)\n",
    "\n",
    "# Check the result\n",
    "print(df['Age'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c3506",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Type' & 'Fatal Y/N'\n",
    "**Fatality depends on Type of injury and severity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def classify_injury(injury, fatal):\n",
    "    if pd.isna(injury):\n",
    "        injury = \"\"\n",
    "    injury = injury.lower()\n",
    "    fatal = str(fatal).strip().upper()\n",
    "\n",
    "    # HOAX / FALSE\n",
    "    if \"hoax\" in injury or \"false\" in injury:\n",
    "        return \"Hoax / False report\"\n",
    "\n",
    "    # NOT A SHARK\n",
    "    if any(x in injury for x in [\"not a shark\", \"stingray\", \"barracuda\", \"propeller\", \"fish\"]):\n",
    "        return \"Not a shark\"\n",
    "\n",
    "    # NO INJURY\n",
    "    if \"no injury\" in injury or \"no injuries\" in injury:\n",
    "        return \"No injury\"\n",
    "\n",
    "    # PROVOKED\n",
    "    if \"provoked\" in injury:\n",
    "        return \"Provoked incident\"\n",
    "\n",
    "    # MISSING\n",
    "    if any(x in injury for x in [\"missing\", \"disappeared\", \"body not recovered\"]):\n",
    "        return \"Missing / Unknown\"\n",
    "\n",
    "    # FATAL CASES\n",
    "    if fatal == \"Y\":\n",
    "        # drowned then bitten after death\n",
    "        if \"post-mortem\" in injury or (\"drown\" in injury and \"post\" in injury):\n",
    "            return \"Fatal | Drowned, shark scavenged\"\n",
    "        if \"unconfirmed\" in injury or \"probable\" in injury:\n",
    "            return \"Fatal | Unconfirmed shark involvement\"\n",
    "        return \"Fatal | Shark confirmed\"\n",
    "\n",
    "    # NON-FATAL CASES\n",
    "    if fatal == \"N\":\n",
    "        if any(x in injury for x in [\"bitten\", \"lacerat\", \"puncture\", \"wound\", \"abrasion\"]):\n",
    "            return \"Non-fatal | Confirmed shark bite\"\n",
    "        # Text but no bite\n",
    "        return \"Non-fatal | Other\"\n",
    "\n",
    "    # Unknown\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"Type\"] = df.apply(lambda row: classify_injury(row[\"Injury\"], row[\"Fatal Y/N\"]), axis=1)\n",
    "\n",
    "\n",
    "# Check RESULTS--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Count number of records in each injury type\n",
    "counts = df[\"Type\"].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = (counts / len(df)) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    \"Cases\": counts,\n",
    "    \"Percentage\": percentages.round(2)\n",
    "})\n",
    "\n",
    "# Save summary to CSV\n",
    "summary.to_csv(\"injury_type_summary.csv\", index=True)\n",
    "\n",
    "# Print SUMMARY\n",
    "# print(summary)\n",
    "\n",
    "print(\"Injury Type Summary:\\n\")\n",
    "for category in counts.index:\n",
    "    print(f\"{category}: {counts[category]} cases  ({percentages[category]:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3514a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c2796",
   "metadata": {},
   "source": [
    "#### CLASSIFY INJURY SEVERITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca64eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to classify injury severity\n",
    "def classify_severity(val):\n",
    "    if pd.isna(val):\n",
    "        val = \"\"\n",
    "    val = val.lower()\n",
    "    \n",
    "    if re.search(r'fatal|death|died|deceased', val):\n",
    "        return \"Death\"\n",
    "    if re.search(r'amputation|severed|severely bitten', val):\n",
    "        return \"Severe injury\"\n",
    "    if re.search(r'bitten|bite|laceration|cuts|fracture', val):\n",
    "        return \"Minor injury\"\n",
    "    if re.search(r'no injury|not injured|unharmed', val):\n",
    "        return \"No injury\"\n",
    "    if re.search(r'possible drowning|unknown', val):\n",
    "        return \"Uncertain\"\n",
    "    \n",
    "    return \"Other/Unknown\"\n",
    "\n",
    "# Apply to create Injury_Severity column\n",
    "df[\"Injury_Severity\"] = df[\"Injury\"].apply(classify_severity)\n",
    "\n",
    "# Now derive Fatal Y/N based on Injury_Severity\n",
    "def derive_fatal(severity):\n",
    "    if severity == \"Death\":\n",
    "        return \"Y\"\n",
    "    elif severity in [\"Severe injury\", \"Injury\", \"No injury\", \"Uncertain\", \"Other/Unknown\"]:\n",
    "        return \"N\"\n",
    "    return \"N\"\n",
    "\n",
    "df[\"Fatal Y/N\"] = df[\"Injury_Severity\"].apply(derive_fatal)\n",
    "\n",
    "# Check the first rows\n",
    "print(df[[\"Injury\", \"Injury_Severity\", \"Fatal Y/N\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c52fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fatal Y/N'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
